Questions For Rebuttal:


Reviewer 1

"The main innovation and contributions are not clear..."

While APA algorithms have been around for 30 years, they have only ever been of theoretical interest, as a step towards reducing the computational complexity exponent of matrix multiplication.  We believe a central contribution of our paper is that we are the first to demonstrate the practicality of the algorithms, providing an efficient, parallel implementation and studying the impact of the approximation error in a practical scenario. 

"In the sequential implementation, it is shown that the highest performing algorithm is ⟨4, 4, 4⟩, is there any reason for this?"

The (4,4,4) algorithm has the greatest reduction in flops (per Table 1); in the sequential case, the performance matches the theoretical reduction in flops reasonably well.

Reviewer 2

Do you have the ability to test a chip that's less than 5 years old?:

We have newer Intel machines available to us, but none of them support half-precision floating point operations (only storage benefits are available).  While upcoming chips may add more support for half-precision instructions, we think there are many use cases of neural network training where half precision is not available, and using APA algorithms provides a benefit as demonstrated in the paper.

"How does compare APA to modern processor support for 16b precision, and have you proven that these two techniques are composable?"

The techniques are certainly composable, and the behavior of APA matrix multiplication error in half precision is predictable, given by the theoretical analysis and confirmed by our empirical evidence in single precision. However, the tolerance of a neural network to composing these two optimizations has not been tested.  The matrix multiplication accuracy of APA algorithms in half-precision would be on the range of 3-5 bits, or 1-2 decimal digits.

Reviewer 3

"The figure in Figure 4 are too crowded to see any thing." (also mentioned by Reviewer 2)

The goal of Figure 4 is to demonstrate that the training and test accuracies are maintained over all algorithms, including classical -- the approximations errors of APA algorithms do not adversely affect network accuracies.

***************************

Detailed Comments:
The main innovation and contributions can not be found clearly in this paper.:
- These algorithms have been mostly theoretical and we are getting real value out of them
- Neural network training is very computationally expensive and is being used everywhere by every major company and we have found a way to speed up this training
    - saves money, time, etc etc tens of millions of dollars are spent on network training etc
    - mlperf (now called ml commons): https://mlcommons.org/en/#philosophy
        - lots of companies want to speed up NN training

In the experiment parts, though a lot of APA methods are tested, but it lacks of further analysis of different APA methods.


The experimental results using a discontinued 6-core Intel CPU from 2012, which provides a rather modest amount of parallelism for experiments in a Parallel Processing conference i 2021

Figure 4: where is "Classical" on the plot? It's impossible to see. The figure in Figure 4 are too crowed to see any thing. Try to use table or make it readable.
- its essentially in the middle of the other algorithms and we used that graph to show that there isn't much difference between model performance


More applications should be introduced to show the robustness.
- There are many other NN applications
    - large language models and other CV models like VGG