\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{tikz,pgfplots}

% for subfigures
\usepackage[caption=false,font=footnotesize]{subfig}

% for scientific notation like 1e-8
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand*{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}} } 
\newcommand{\dims}[1]{\langle #1 \rangle}

\newcommand{\datafile}{}

\newcommand{\GB}[1]{\textcolor{red}{\textbf{GB}: #1}}
\newcommand{\JW}[1]{\textcolor{blue}{\textbf{JW}: #1}}
\newcommand{\RZ}[1]{\textcolor{purple}{\textbf{RZ}: #1}}

\begin{document}

\title{Accelerating Neural Network Training using Arbitrary Precision Approximating Matrix Multiplication Algorithms
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Grey Ballard}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Wake Forest University}\\
Winston Salem, NC, USA \\
\email{ballard@wfu.edu}}
\and
\IEEEauthorblockN{Jack Weissenberger}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Wake Forest University}\\
Winston Salem, NC, USA \\
\email{jack.weissenberger@gmail.com}}
\and
\IEEEauthorblockN{Luoping\GB{?} Zhang}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Wake Forest University}\\
Winston Salem, NC, USA \\
\email{zhanl317@wfu.edu}}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}



\emph{Fast} matrix multiplication algorithms are those that perform fewer than the $2n^3+O(n^2)$ flops performed by the classical algorithm.
For example, Strassen's original fast matrix multiplication algorithm performs $O(n^{\log_2 7})$ flops, where $\log_27 \approx 2.81$ \cite{Strassen69}.
The true complexity of matrix multiplication, typically measured as the exponent $\omega$ for complexity $O(n^\omega)$ is an open question, but the current tightest upper bound is 2.37286 \cite{AW21}.
Upper bounds in this range correspond to theoretical algorithms that are not expected to be practical.

One of the reasons such algorithms are impractical is that they are based on so-called \emph{Arbitrary Precision Approximating} (APA) algorithms, which we describe in detail in \cref{sec:APA}.
In exact arithmetic, these algorithms compute an approximation of the correct result, where the error is polynomial in a nonzero parameter of the algorithm \cite{BLR80}.
That is, in exact arithmetic, the error can be made arbitrarily small.
In floating point arithmetic, however, there is a lower bound on the approximation error that depends on the working precision and properties of the algorithm.
In theory, APA algorithms can be converted to exact algorithms at the cost of an extra logarithmic factor of $n$, which is typically hidden by an arbitrarily small increase in the exponent \cite{Bini80}.
In practice, however, $n$ is not large enough to ignore the logarithmic factor, and floating point error plagues the conversion method, so APA algorithms are considered approximate algorithms with insufficient accuracy for most applications.
Thus, APA algorithms are largely overlooked as practical tools despite their performance potential \cite{BB15}.

Our goal in this paper is to demonstrate that APA algorithms can offer practical performance improvements for applications that are tolerant to error in matrix multiplications, notably the training phase of neural networks.
Training large neural networks is computationally expensive and the cost has spurred a surge of research into more efficient hardware, better algorithms, and techniques for trading off accuracy for performance.
For instance, low-precision arithmetic has been shown to decrease running time with little to no effect on the ultimate learning task \cite{GAGN15,HCSEB17}, and new floating point formats have been developed and supported in hardware to implement highly efficient low- and mixed-precision computation \cite{KM+19,YWC20}.

Matrix multiplication in particular is a bottleneck computation for many neural networks... \GB{continue here, make the case that matmul is bottleneck}

\GB{reference this paper \cite{KAA20} as using Strassen-Winograd in a single-layer feed-forward network on a GPU}

\GB{reference this paper \cite{NPOV15} for tensorizing large fully-connected layers, including layers with dimensions 25K x 4K and 4K x 4K}

\GB{list of contributions:}

\begin{enumerate}
	\item curating a collection of recently discovered practical APA algorithms
	\item extending the framework of \cite{BB15} to generate efficient multithreaded code for a more general set of APA algorithms
	\item demonstrating the robustness of learning accuracy to approximate matrix multiplications
	\item presenting multithreaded performance improvements of a synthetic MLP of up to $Y\times$
\end{enumerate}

\section{Arbitrary Precision Approximating Algorithms}
\label{sec:APA}

\begin{enumerate}
	\item complexity of matmul discussion
	\item describe nature of approximation
	\item give explicit algorithm (Bini?)
	\item table of 12 algorithms
	\item accuracy plot?
\end{enumerate}



\GB{cite \cite{Bini80} for conversion of APA algorithm to exact algorithm}

\begin{table}
\centering
\caption{Properties of Arbitrary Precision Approximating Algorithms  \\ Speedup and Error are computed assuming 1 recursive step}
\label{tab:algs}
\begin{tabular}{| c | c c c | c c c |} 
\hline
\textbf{Ref} & \textbf{Dims} & \textbf{Rank} & \textbf{Speedup} & $\mathbf{\sigma}$ & $\mathbf{\varphi}$ & \textbf{Error} \\
\hline
- & $\dims{2,2,2}$ & 8 & - & 1 & 0 & \num{1.2e-7} \\
\hline
\cite{BCRL79} & $\dims{3,2,2}$ & 10 & $20\%$ & 1 & 1 & \num{3.5e-4} \\
\cite{AS13} & $\dims{4,2,2}$ & 13 & $23\%$ & 1 & 2 & \num{4.9e-3} \\
\cite{Smirnov13} & $\dims{3,3,2}$ & 14 & $29\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov13} & $\dims{5,2,2}$ & 16 & $25\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov13} & $\dims{3,3,3}$ & 20 & $35\%$ & 1 & 6 & \num{1.0e-1} \\
\cite{Schonhage81} & $\dims{3,3,3}$ & 21 & $29\%$ & 1 & 2 & \num{4.9e-3} \\
\cite{Smirnov15} & $\dims{7,2,2}$ & 22 & $27\%$ & 1 & 5 & \num{7.0e-2} \\
\cite{Smirnov16b} & $\dims{4,4,2}$ & 24 & $33\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov16a} & $\dims{4,3,3}$ & 27 & $33\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov16b} & $\dims{5,5,2}$ & 37 & $35\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov14} & $\dims{4,4,4}$ & 46 & $39\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov18} & $\dims{5,5,5}$ & 90 & $39\%$ & 1 & 3 & \num{1.9e-2} \\
\hline
\end{tabular}
\end{table}

\GB{specify formula for speedup and error values...}

\section{Parallel Fast Matrix Multiplication}

\GB{reference PPoPP paper: \cite{BB15}}

\begin{enumerate}
	\item describe code generation and hybrid parallelization
	\item give performance plots (sequential and parallel)
\end{enumerate}

\section{Neural Network Accuracy}

\begin{enumerate}
	\item describe network (MNIST?) and implementation details
	\item accuracy results in table or fig
\end{enumerate}

\section{Neural Network Performance}

\begin{enumerate}
	\item describe synthetic network and implementation details
	\item perf results for growing dim (maybe for serial and 12-threads separately)
	\item parallel speedup results
\end{enumerate}

\begin{figure}
\subfloat[One thread]{
\centering
\renewcommand{\datafile}{data/NNtime_1t.dat}
\input{plots/reltime}
\label{fig:NNreltime_seq}}
\\
\subfloat[Twelve threads]{
\centering
\renewcommand{\datafile}{data/NNtime_12t.dat}
\input{plots/reltime}
\label{fig:NNreltime_par}}
\caption{Network training time relative to using classical matrix multiplication}
\label{fig:NNreltime}
\end{figure}



\section{Conclusion}

\begin{enumerate}
	\item promising results for synthetic network, goal is to target costly networks bottlenecked by matmul
	\item can tune for matmul dimensions (not just square)
	\item GPU implementation (cite Jianyu Huang's work)
	\item mixed precision fast matmul (APA or otherwise)
\end{enumerate}

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
