\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{tikz,pgfplots}

% for subfigures
\usepackage[caption=false,font=footnotesize]{subfig}

% for scientific notation like 1e-8
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand*{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}} } 
\newcommand{\dims}[1]{\langle #1 \rangle}

\newcommand{\datafile}{}

\newcommand{\GB}[1]{\textcolor{red}{\textbf{GB}: #1}}
\newcommand{\JW}[1]{\textcolor{blue}{\textbf{JW}: #1}}
\newcommand{\RZ}[1]{\textcolor{purple}{\textbf{RZ}: #1}}

\begin{document}

\title{Accelerating Neural Network Training using Arbitrary Precision Approximating Matrix Multiplication Algorithms
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Grey Ballard}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Wake Forest University}\\
Winston Salem, NC, USA \\
\email{ballard@wfu.edu}}
\and
\IEEEauthorblockN{Jack Weissenberger}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Wake Forest University}\\
Winston Salem, NC, USA \\
\email{jack.weissenberger@gmail.com}}
\and
\IEEEauthorblockN{Luoping\GB{?} Zhang}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Wake Forest University}\\
Winston Salem, NC, USA \\
\email{zhanl317@wfu.edu}}
}

\maketitle

\begin{abstract}
\GB{TBD}
\end{abstract}

\section{Introduction}

\emph{Fast} matrix multiplication algorithms are those that perform fewer than the $2n^3+O(n^2)$ flops performed by the classical algorithm.
For example, Strassen's original fast matrix multiplication algorithm performs $O(n^{\log_2 7})$ flops, where $\log_27 \approx 2.81$ \cite{Strassen69}.
The true complexity of matrix multiplication, typically measured as the exponent $\omega$ for complexity $O(n^\omega)$ is an open question, but the current tightest upper bound is 2.37286 \cite{AW21}.
Upper bounds in this range correspond to theoretical algorithms that are not expected to be practical.

One of the reasons such algorithms are impractical is that they are based on so-called \emph{Arbitrary Precision Approximating} (APA) algorithms, which we describe in detail in \cref{sec:APA}.
In exact arithmetic, these algorithms compute an approximation of the correct result, where the error is polynomial in a nonzero parameter of the algorithm \cite{BLR80}.
That is, in exact arithmetic, the error can be made arbitrarily small.
In floating point arithmetic, however, there is a lower bound on the approximation error that depends on the working precision and properties of the algorithm.
Thus, APA algorithms are often considered to have insufficient accuracy for most applications and have largely been overlooked as practical tools despite their performance potential and ability to outperform their exact counterparts \cite{BB15}.

Our goal in this paper is to demonstrate that APA algorithms can offer practical performance improvements for applications that are tolerant to error in matrix multiplications, notably the training phase of neural networks.
Training large neural networks is computationally expensive and the cost has spurred a surge of research into more efficient hardware, better algorithms, and techniques for trading off accuracy for performance.
For instance, low-precision arithmetic has been shown to decrease running time with little to no effect on the ultimate learning task \cite{GAGN15,HCSEB17}, and new floating point formats have been developed and supported in hardware to implement highly efficient low- and mixed-precision computation \cite{KM+19,YWC20}.

Matrix multiplication in particular is a bottleneck computation for many neural networks.
Forward and backward propagation in training the weights of fully connected layers is a matrix multiplication with dimensions given by the sizes of the layers and the size of the batch.
Training convolutional and other types of layers can also be cast as matrix multiplication, either via monolithic multiplications or batches of smaller multiplications \cite{CW+14,GB+19}.
In this paper we focus on Multi-Layer Perceptron (MLP) networks that rely on a sequence of fully connected layers \cite{HSW89}.
Because the sizes of the layers in MLP networks continue to grow, various techniques have been used to reduce the computational demands of the training phase.
For example, low-rank tensor approximation of the weights can reduce both memory and computation \cite{NPOV15}, and fast matrix multiplication (the Strassen-Winograd algorithm) has been applied to the bottleneck matrix multiplications \cite{KAA20}.

Our contribution is the use of APA matrix multiplication algorithms to address this problem and accelerate training.
In particular we
\begin{enumerate}
	\item curate a collection of well-established and recently discovered practical APA algorithms;
	\item extend the framework of \cite{BB15} to generate efficient multithreaded code for all of them;
	\item demonstrate the robustness of learning accuracy to approximate matrix multiplications; and
	\item present multithreaded performance improvements of a synthetic MLP of up to \GB{$Y\times$}.
\end{enumerate}

\GB{\cite{NPOV15} uses a network including layers with dimensions 25K x 4K and 4K x 4K}

\section{Practical Arbitrary Precision Approximating Matrix Multiplication Algorithms}
\label{sec:APA}

\subsection{Fast Matrix Multiplication}

Nearly all fast matrix multiplication algorithms are based a rule for multiplying matrices of fixed size, and the reduction in asymptotic complexity stems from using the rule recursively on general matrices.
For example, Strassen's algorithm is specified by a rule for multiplying two $2\times2$ matrices (denoted $\dims{2,2,2}$) using 7 multiplications instead of the classical algorithm's 8 multiplications.
Applying the rule to $n\times n$ matrices, we split each matrix into quadrants, and the 7 multiplications are multiplications of $(n/2)\times (n/2)$ matrices.
For even better efficiency, we can consider larger fixed sizes and find rules that require a lower percentage of multiplications compared to the classical rule.
The number of multiplications in a rule is known as the \emph{rank}, so Strassen's is a rank-7 algorithm.
Algorithms have been derived both analytically and computationally, and there exists a vast set of improvements leading to the current world record \cite{Pan84,CW87,Williams12,AW21}.

\subsection{APA Algorithms}

A key characteristic of matrix multiplication, first demonstrated by Bini et al.~\cite{BCRL79}, is that it can be approximated to arbitrary accuracy more efficiently than being computed exactly.
An APA algorithm is one that takes as input $A$, $B$, and a parameter $0<\lambda<1$ and computes 
\begin{equation}
\label{eq:APAapprox}
\hat C = A\cdot B + \lambda E + O(\lambda^2).
\end{equation}
Thus, in exact arithmetic, letting $\lambda \rightarrow 0$ achieves arbitrarily small approximation error.
In floating point arithmetic, choosing too small a value for $\lambda$ leads to accumulation of roundoff error that exceeds the approximation error.
We discuss optimizing $\lambda$ in \cref{sec:APAerr}.

In theory, APA algorithms can be converted to exact algorithms at the cost of an extra logarithmic factor of $n$, which is typically hidden by an arbitrarily small increase in the exponent \cite{Bini80}.
In practice, $n$ is not large enough to ignore the logarithmic factor, so we consider each APA algorithm as is.

For a concrete example, we reproduce the rule for the algorithm developed by Bini et al.~\cite{BCRL79} for the $\dims{3,2,2}$ case (multiplying a $3\times 2$ matrix $A$ by a $2\times 2$ matrix $B$), where we use the following notation for input and output matrices:
$$\begin{bmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \\ C_{31} & C_{32} \end{bmatrix} = 
\begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \\ A_{31} & A_{32}  \end{bmatrix} \cdot
\begin{bmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{bmatrix}.$$
Bini's rule is given by
\begin{align*}
M_1 =& (A_{11} + A_{22}) \cdot (\lambda B_{11} + B_{22}) \\
M_2 =& A_{22}\cdot  (-B_{21} - B_{22}) \\
M_3 =& A_{11}\cdot B_{22} \\
M_4 =& (\lambda A_{12} + A_{22})\cdot (-\lambda B_{11} + B_{21}) \\
M_5 =& (A_{11} + \lambda A_{12}) \cdot (\lambda B_{12} + B_{22}) \\
M_6 =& (A_{21} + A_{32}) \cdot (B_{11} + \lambda B_{22}) \\
M_7 =& A_{21} \cdot (-B_{11} - B_{12}) \\
M_8 =& A_{32} \cdot B_{11}\\
M_9 =& (A_{21} + \lambda A_{31}) \cdot (B_{12} - \lambda B_{22})\\
M_{10} =& (\lambda A_{31} + A_{32}) \cdot (B_{12} - \lambda B_{22})\\
\end{align*}
\begin{align*}
C_{11} =& \lambda^{-1} (M_1 + M_2 - M_3 + M_4) \\
C_{12} =& \lambda^{-1} (-M_3 + M_5) \\
C_{21} =& M_4 + M_6 - M_{10}\\
C_{22} =& M_1 - M_5 + M_9\\
C_{31} =& \lambda^{-1} (-M_8 + M_{10}) \\
C_{32} =& \lambda^{-1} (M_6 + M_7 - M_8 + M_9).\\
\end{align*}

We highlight several properties of the rule which are common across all APA algorithms we consider.
First, the rule requires fewer multiplications than the classical one (rank 10 instead of 12 for $\dims{3,2,2}$ here).
Next, each multiplication is between a linear combination of entries of $A$ and a linear combination of entries of $B$, each output entry is computed as a linear combination of the outputs of the multiplications, and each coefficient in the linear combinations is a (Laurent) polynomial in $\lambda$.
The coefficients include both positive and negative powers of $\lambda$, which explains why small values of $\lambda$ can lead to significant roundoff error.
For Bini's algorithm all coefficients are monomial with degree between $-1$ and $1$.

Because of this general pattern, we can encode APA and other fast algorithms succinctly by their linear combination coefficients.
For example, encoding the first multiplication $M_1$ in Bini's algorithm can be done using a triplet of matrices:
\begin{equation}
\label{eq:triplet}
\begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}, \hfill
\begin{bmatrix} \lambda & 0 \\ 0 & 1 \end{bmatrix}, \hfill
\begin{bmatrix} \lambda^{-1} & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}.
\end{equation}
The first two matrices specify the linear combinations taken of entries of $A$ and $B$, and the third matrix specifies the contributions of $M_1$ to the entries of $C$.
Ten such triplets completely specify Bini's algorithm.

\subsection{Numerical Error of APA Algorithms}
\label{sec:APAerr}

\GB{should we add an accuracy plot in this section?}

In floating point arithmetic, the lower bound on the numerical error of APA algorithms depends on the working precision and two parameters of the algorithm \cite{BLR80}.
The working precision, also referred to as machine precision, is the upper bound on relative error incurred by basic operations in a given floating point format and depends on the number of fractional bits used in the format.
We use the notation $2^{-d}$ for working precision, where $d=52$ for double and $d=23$ for single precision (note that $2^{-52}\approx 10^{-16}$ and $2^{-23}\approx 10^{-7}$).

The two parameters of the APA algorithm specify the contribution of the approximation and roundoff errors, respectively.
The first parameter, $\sigma$, is the smallest positive exponent of the error polynomial and represents the approximation error.
\Cref{eq:APAapprox} shows the error as a polynomial of $\lambda$ whose leading term is linear in $\lambda$.
If an algorithm satisfies \cref{eq:APAapprox} with $E\neq 0$, then $\sigma=1$.
However, if $E=0$, then $\sigma>1$ is the degree of the leading monomial.
Larger $\sigma$ implies smaller error due to the algorithm, though all of the APA algorithms we consider have $\sigma=1$.

The second parameter, $\varphi$, is the largest (in absolute value) negative exponent of the algorithm, computed as the largest sum of negative exponents across all triplets of matrices.
This parameter represents the effect of roundoff error caused by floating point arithmetic involving the largest intermediate values computed by the algorithm.
For example, the triplet given in \cref{eq:triplet} yields a sum of negative exponents of $0+0+1=1$, and in the case of Bini's algorithm, no other triplet has a larger sum, so $\varphi=1$ for that algorithm.
Smaller $\varphi$ implies smaller error due to roundoff, and the APA algorithms we consider exhibit a range of values.

Given these two contributions to the numerical error, $\lambda$ can be optimized to balance the effects based on parameters $\sigma$ and $\varphi$ (and $d$).
As shown by Bini, Lotti, and Romani \cite{BLR80}, the optimal $\lambda$ should be set to $\Theta(2^{-d/(\sigma+\varphi)})$.
Using this value of $\lambda$, the numerical error incurred by the algorithm will be bounded by $O(2^{-d\sigma/(\sigma+\varphi)})$.
Taking Bini's algorithm as an example, we have $\sigma=\varphi=1$, so the error is $O(2^{-d/2})$, or the square root of working precision.
Note that if multiple recursive steps are used, then $\varphi$ increases proportional to the number of steps, so straightforward optimization of $\lambda$ results in error of $O(2^{-d\sigma/(\sigma+s\varphi)})$ for $s$ recursive steps.

\subsection{Practical Algorithms}

We are particularly interested in algorithms with rules for \emph{small} fixed sizes because they have more promise for practical performance.
This is because larger fixed sizes result in multiplications of small submatrices, and matrix multiplication performance degrades for smaller dimensions.
For instance, consider a rule for dimensions $\dims{4,4,4}$ applied to matrices of reasonable size, less than dimension $10{,}000$.
After one recursive call, the submatrices are of size less than 2500, and after two recursive calls, the dimension is less than 625.
At this size, the reduction in number of flops is offset by a reduction in performance, which may result in longer running time.

Instead of focusing on the exponent of the asymptotic complexity of fast algorithms, for practical algorithms we are more interested in the constant reduction in flops of a single recursive level (a single use of the rule of the algorithm).
This is because in practice, for reasonable matrix dimensions (no more than 10,000, say), only 1 or 2 recursive levels will yield performance improvement \cite{BB15}.
We also prefer algorithms with fewer nonzero coefficients in the linear combinations, because while less costly than multiplications, the matrix additions are less efficient (they are memory bandwidth bound) and prevent achieving the ideal speedup given by the reduction in multiplications.
For dimensions $\dims{m,n,k}$ and rank $r$, the ideal speedup for a single recursive step is given by $mnk/r$, and two recursive steps would enable a possible speedup of $(mnk/r)^2$.
These speedups are typically not fully attained because of degradation in performance for smaller matrix dimensions and the overhead of matrix additions.
In the experimental results of this work, we use only 1 recursive step for all algorithms.

\subsection{APA Algorithm Properties}

\Cref{tab:algs} shows the key performance and accuracy properties of the APA algorithms we consider.
Each row corresponds to an algorithm, and the first row includes the classical algorithm for comparison.
For all algorithms, we assume only 1 recursive step is used, though the speedup and error for more steps can be readily calculated from the parameters.
The first column gives the reference where the algorithm was first specified.
The second block column demonstrates the possible performance improvement, where speedup is calculated as $(mnk/r - 1)\cdot100\%$ for $\dims{m,n,k}$ and rank $r$.
The third block column shows error parameters, with error calculated as $2^{-d\cdot\sigma/(\sigma+\varphi)}$ with $d=23$, corresponding to single precision.

\begin{table}
\centering
\caption{Properties of Arbitrary Precision Approximating Algorithms  \\ Speedup and Error are computed assuming 1 recursive step}
\label{tab:algs}
\begin{tabular}{| c | c c c | c c c |} 
\hline
\textbf{Ref} & \textbf{Dims} & \textbf{Rank} & \textbf{Speedup} & $\mathbf{\sigma}$ & $\mathbf{\varphi}$ & \textbf{Error} \\
\hline
- & $\dims{2,2,2}$ & 8 & - & 1 & 0 & \num{1.2e-7} \\
\hline
\cite{BCRL79} & $\dims{3,2,2}$ & 10 & $20\%$ & 1 & 1 & \num{3.5e-4} \\
\cite{AS13} & $\dims{4,2,2}$ & 13 & $23\%$ & 1 & 2 & \num{4.9e-3} \\
\cite{Smirnov13} & $\dims{3,3,2}$ & 14 & $29\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov13} & $\dims{5,2,2}$ & 16 & $25\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov13} & $\dims{3,3,3}$ & 20 & $35\%$ & 1 & 6 & \num{1.0e-1} \\
\cite{Schonhage81} & $\dims{3,3,3}$ & 21 & $29\%$ & 1 & 2 & \num{4.9e-3} \\
\cite{Smirnov15} & $\dims{7,2,2}$ & 22 & $27\%$ & 1 & 5 & \num{7.0e-2} \\
\cite{Smirnov16b} & $\dims{4,4,2}$ & 24 & $33\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov16a} & $\dims{4,3,3}$ & 27 & $33\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov16b} & $\dims{5,5,2}$ & 37 & $35\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov14} & $\dims{4,4,4}$ & 46 & $39\%$ & 1 & 3 & \num{1.9e-2} \\
\cite{Smirnov18} & $\dims{5,5,5}$ & 90 & $39\%$ & 1 & 3 & \num{1.9e-2} \\
\hline
\end{tabular}
\end{table}



\section{Parallel Fast Matrix Multiplication}

\GB{reference PPoPP paper: \cite{BB15}}

\begin{enumerate}
	\item describe code generation and hybrid parallelization
	\item give performance plots (sequential and parallel)
\end{enumerate}

\section{Neural Network Accuracy}

\begin{enumerate}
	\item describe network (MNIST?) and implementation details
	\item accuracy results in table or fig
\end{enumerate}

\section{Neural Network Performance}

\begin{enumerate}
	\item describe synthetic network and implementation details
	\item perf results for growing dim (maybe for serial and 12-threads separately)
	\item parallel speedup results
\end{enumerate}

\begin{figure}
\subfloat[One thread]{
\centering
\renewcommand{\datafile}{data/NNtime_1t.dat}
\input{plots/reltime}
\label{fig:NNreltime_seq}}
\\
\subfloat[Twelve threads]{
\centering
\renewcommand{\datafile}{data/NNtime_12t.dat}
\input{plots/reltime}
\label{fig:NNreltime_par}}
\caption{Network training time relative to using classical matrix multiplication}
\label{fig:NNreltime}
\end{figure}



\section{Conclusion}

\begin{enumerate}
	\item promising results for synthetic network, goal is to target costly networks bottlenecked by matmul
	\item can tune for matmul dimensions (not just square)
	\item GPU implementation (cite Jianyu Huang's work)
	\item mixed precision fast matmul (APA or otherwise)
\end{enumerate}

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
